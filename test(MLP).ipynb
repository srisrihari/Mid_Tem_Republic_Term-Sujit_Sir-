{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.11.11' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'thyroid_cancer_risk_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separate binary and non-binary categorical columns\n",
    "binary_cols = ['Gender', 'Family_History', 'Radiation_Exposure', 'Iodine_Deficiency', 'Smoking', 'Obesity', 'Diabetes']\n",
    "non_binary_cols = ['Country', 'Ethnicity']\n",
    "\n",
    "# Label encoding for binary columns\n",
    "label_encoders = {}\n",
    "for column in binary_cols:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# One-hot encoding for non-binary categorical columns\n",
    "one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "transformed_cols = one_hot_encoder.fit_transform(data[non_binary_cols])\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded columns\n",
    "one_hot_df = pd.DataFrame(transformed_cols, columns=one_hot_encoder.get_feature_names_out(non_binary_cols))\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original data\n",
    "data = pd.concat([data.drop(non_binary_cols, axis=1), one_hot_df], axis=1)\n",
    "\n",
    "# Convert the target variable to numeric\n",
    "data['Diagnosis'] = data['Diagnosis'].map({'Benign': 0, 'Malignant': 1})\n",
    "\n",
    "# Create interaction terms\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "interaction_features = poly.fit_transform(data.drop(['Patient_ID', 'Thyroid_Cancer_Risk', 'Diagnosis'], axis=1))\n",
    "\n",
    "# Create a DataFrame with the interaction features\n",
    "interaction_df = pd.DataFrame(interaction_features, columns=poly.get_feature_names_out(data.drop(['Patient_ID', 'Thyroid_Cancer_Risk', 'Diagnosis'], axis=1).columns))\n",
    "\n",
    "# Concatenate the interaction features with the original data\n",
    "data = pd.concat([data, interaction_df], axis=1)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(['Patient_ID', 'Thyroid_Cancer_Risk', 'Diagnosis'], axis=1)\n",
    "y = data['Diagnosis']\n",
    "\n",
    "# Split the data into training and testing sets with stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance using ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_res, y_train_res = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the MLP model with dropout and L2 regularization\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.5, activation=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        previous_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(previous_size, hidden_size))\n",
    "            layers.append(activation())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            previous_size = hidden_size\n",
    "        layers.append(nn.Linear(previous_size, output_size))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train_res.shape[1]\n",
    "hidden_sizes = [128, 64, 32]  # Configurable hidden layers\n",
    "output_size = 1\n",
    "model = MLP(input_size, hidden_sizes, output_size, dropout_rate=0.5)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)  # Lower learning rate and L2 regularization\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_res, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_res.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "    y_pred_prob = test_outputs.round()\n",
    "    accuracy = accuracy_score(y_test_tensor, y_pred_prob)\n",
    "    precision = precision_score(y_test_tensor, y_pred_prob)\n",
    "    recall = recall_score(y_test_tensor, y_pred_prob)\n",
    "    f1 = f1_score(y_test_tensor, y_pred_prob)\n",
    "    roc_auc = roc_auc_score(y_test_tensor, y_pred_prob)\n",
    "    conf_matrix = confusion_matrix(y_test_tensor, y_pred_prob)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test Precision: {precision:.4f}')\n",
    "    print(f'Test Recall: {recall:.4f}')\n",
    "    print(f'Test F1-Score: {f1:.4f}')\n",
    "    print(f'Test ROC-AUC: {roc_auc:.4f}')\n",
    "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'mlp_model.pth')\n",
    "\n",
    "# CLI Application\n",
    "import argparse\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = MLP(input_size, hidden_sizes, output_size, dropout_rate=0.5)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict(model, data):\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data_tensor)\n",
    "        predictions = outputs.round().numpy()\n",
    "    return predictions\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Thyroid Cancer Risk Prediction CLI')\n",
    "    parser.add_argument('--input', type=str, required=True, help='Path to the input CSV file')\n",
    "    parser.add_argument('--output', type=str, required=True, help='Path to save the output CSV file')\n",
    "    parser.add_argument('--model', type=str, default='mlp_model.pth', help='Path to the trained model file')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(args.model)\n",
    "\n",
    "    # Load the input data\n",
    "    input_data = pd.read_csv(args.input)\n",
    "    input_data = scaler.transform(input_data)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = predict(model, input_data)\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    output_df = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "    output_df.to_csv(args.output, index=False)\n",
    "    print(f'Predictions saved to {args.output}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
